{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avk1943/MS-Project/blob/main/MS_Code_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNxWksq0Bi13"
      },
      "source": [
        "# Preparing Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chJrBTqB_Yhw",
        "outputId": "504c4ce7-9cdc-45e6-f8ea-c15cd3130106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uyPoOPsKoXtm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIOhuQ4c4R-1",
        "outputId": "311728a5-ccb2-409c-d6c5-a777fba5d893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shared drives/Master Project\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/drive/Shared drives/Master Project/\n",
        "\n",
        "with open('master_data_X.npy', 'rb') as f:\n",
        "  X = np.load(f)\n",
        "\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wWv89MT52Bk"
      },
      "outputs": [],
      "source": [
        "with open('master_data_Y.npy', 'rb') as f:\n",
        "  y = np.load(f)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBdRQjw981-Q"
      },
      "outputs": [],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bAbbEn868a"
      },
      "outputs": [],
      "source": [
        "y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijVlEc0E9eza"
      },
      "outputs": [],
      "source": [
        "X = np.float32(X)\n",
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDSeLAm79YAH"
      },
      "outputs": [],
      "source": [
        "y = np.float32(y)\n",
        "y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNk2JuEd4eVx"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1HRy1Hi30eb"
      },
      "outputs": [],
      "source": [
        "x_set = torch.from_numpy(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjl_60Qq4njI"
      },
      "outputs": [],
      "source": [
        "x_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxOfkO2Sl1za"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEeIrhZPEJ3c"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.unsqueeze(1)\n",
        "#x_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbYqQTqEAFpR"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEzWmQvd4pud"
      },
      "outputs": [],
      "source": [
        "y_set = torch.from_numpy(y)\n",
        "y_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgfyYVrTl-Fx"
      },
      "outputs": [],
      "source": [
        "#y_set = y_set.type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4JSdfV6niN7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftCogVKw1rN-"
      },
      "outputs": [],
      "source": [
        "data_set = TensorDataset(x_set, y_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEVb6Nr6bsl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8IYm4Ws1rLL"
      },
      "outputs": [],
      "source": [
        "test_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(x_set)\n",
        "print(\"Dataset size:\"+str(dataset_size))\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(test_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRm_BDjBaTXF"
      },
      "outputs": [],
      "source": [
        "print(len(train_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtKCNtJTaXwg"
      },
      "outputs": [],
      "source": [
        "print(len(test_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcD6Fy1862EI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxm2qnHB1rIR"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
        "                                           sampler=train_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n32RRYqpB0dn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWDVw20ZB2sv"
      },
      "source": [
        "# Basic SNN Model with more control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDnIEHOKB8LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ee0067-efae-4933-f865-07a02f126b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/109.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "#batch_size = 128\n",
        "#data_path='/data/mnist'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d286ef9-5fe6-4578-a686-91559a1f81d2"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        num_inputs = 30000\n",
        "        num_hidden = 300\n",
        "        num_outputs = 3\n",
        "        #spike_grad = surrogate.fast_sigmoid()\n",
        "\n",
        "        # global decay rate for all leaky neurons in layer 1\n",
        "        beta1 = 0.9\n",
        "        # independent decay rate for each leaky neuron in layer 2: [0, 1)\n",
        "        beta2 = torch.rand((num_outputs), dtype = torch.float) #.to(device)\n",
        "\n",
        "        # Init layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta1, learn_beta=True)\n",
        "        #self.double()\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Leaky(beta=beta2, learn_beta=True)\n",
        "        #self.double()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # reset hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x.flatten(1))\n",
        "            #print(x.flatten(1).size())\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec), torch.stack(mem2_rec)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz7qFz2GEZzJ"
      },
      "outputs": [],
      "source": [
        "import snntorch.functional as SF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aCrVAh_cyTU",
        "outputId": "71e8ef95-a8f9-4f6e-a398-2b17393920bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 0 \n",
            "Train Loss: 2.16\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 25 \n",
            "Train Loss: 2.00\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 50 \n",
            "Train Loss: 2.12\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 75 \n",
            "Train Loss: 1.39\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 100 \n",
            "Train Loss: 1.62\n",
            "Accuracy: 12.50%\n",
            "\n",
            "Epoch 0, Iteration 125 \n",
            "Train Loss: 1.36\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 150 \n",
            "Train Loss: 0.65\n",
            "Accuracy: 75.00%\n",
            "\n",
            "Epoch 0, Iteration 175 \n",
            "Train Loss: 0.74\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 200 \n",
            "Train Loss: 0.77\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 225 \n",
            "Train Loss: 0.80\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 250 \n",
            "Train Loss: 0.54\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 275 \n",
            "Train Loss: 0.80\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 300 \n",
            "Train Loss: 0.39\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 325 \n",
            "Train Loss: 0.72\n",
            "Accuracy: 12.50%\n",
            "\n",
            "Epoch 0, Iteration 350 \n",
            "Train Loss: 0.47\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 375 \n",
            "Train Loss: 0.31\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 400 \n",
            "Train Loss: 0.64\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 425 \n",
            "Train Loss: 0.41\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 450 \n",
            "Train Loss: 0.51\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 475 \n",
            "Train Loss: 0.46\n",
            "Accuracy: 75.00%\n",
            "\n",
            "Epoch 0, Iteration 500 \n",
            "Train Loss: 0.87\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 525 \n",
            "Train Loss: 0.30\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 550 \n",
            "Train Loss: 0.42\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 575 \n",
            "Train Loss: 0.34\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 600 \n",
            "Train Loss: 0.37\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 625 \n",
            "Train Loss: 0.57\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 650 \n",
            "Train Loss: 0.43\n",
            "Accuracy: 87.50%\n",
            "\n",
            "Epoch 0, Iteration 675 \n",
            "Train Loss: 1.07\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 700 \n",
            "Train Loss: 0.56\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 725 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 750 \n",
            "Train Loss: 0.35\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 775 \n",
            "Train Loss: 0.55\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 800 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 825 \n",
            "Train Loss: 0.28\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 850 \n",
            "Train Loss: 0.37\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 875 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 900 \n",
            "Train Loss: 0.59\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 925 \n",
            "Train Loss: 0.14\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Epoch 0, Iteration 950 \n",
            "Train Loss: 0.38\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 975 \n",
            "Train Loss: 0.47\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1000 \n",
            "Train Loss: 0.62\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1025 \n",
            "Train Loss: 0.49\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1050 \n",
            "Train Loss: 0.40\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1075 \n",
            "Train Loss: 0.22\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Epoch 0, Iteration 1100 \n",
            "Train Loss: 0.47\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1125 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1150 \n",
            "Train Loss: 0.51\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 1175 \n",
            "Train Loss: 0.43\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1200 \n",
            "Train Loss: 0.60\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 1225 \n",
            "Train Loss: 0.51\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1250 \n",
            "Train Loss: 0.22\n",
            "Accuracy: 75.00%\n",
            "\n",
            "Epoch 0, Iteration 1275 \n",
            "Train Loss: 0.94\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1300 \n",
            "Train Loss: 0.40\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1325 \n",
            "Train Loss: 0.25\n",
            "Accuracy: 75.00%\n",
            "\n",
            "Epoch 0, Iteration 1350 \n",
            "Train Loss: 0.38\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1375 \n",
            "Train Loss: 1.33\n",
            "Accuracy: 12.50%\n",
            "\n",
            "Epoch 0, Iteration 1400 \n",
            "Train Loss: 0.77\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1425 \n",
            "Train Loss: 0.37\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1450 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 1475 \n",
            "Train Loss: 0.42\n",
            "Accuracy: 62.50%\n",
            "\n",
            "Epoch 0, Iteration 1500 \n",
            "Train Loss: 0.59\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1525 \n",
            "Train Loss: 0.85\n",
            "Accuracy: 25.00%\n",
            "\n",
            "Epoch 0, Iteration 1550 \n",
            "Train Loss: 0.81\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1575 \n",
            "Train Loss: 0.43\n",
            "Accuracy: 37.50%\n",
            "\n",
            "Epoch 0, Iteration 1600 \n",
            "Train Loss: 0.50\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Epoch 0, Iteration 1625 \n",
            "Train Loss: 0.34\n",
            "Accuracy: 62.50%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.7, incorrect_rate=0.3)\n",
        "#SF.ce_count_loss()\n",
        "#SF.mse_count_loss(correct_rate=0.6, incorrect_rate=0.4)\n",
        "\n",
        "num_epochs = 1\n",
        "num_steps = 10  # run for 25 time steps\n",
        "\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        net.train()\n",
        "        spk_rec, _ = net(data)\n",
        "        loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # print every 25 iterations\n",
        "        if i % 25 == 0:\n",
        "          net.eval()\n",
        "          print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "          # check accuracy on a single batch\n",
        "          acc = SF.accuracy_rate(spk_rec, targets)\n",
        "          acc_hist.append(acc)\n",
        "          print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "\n",
        "        # uncomment for faster termination\n",
        "        # if i == 150:\n",
        "        #     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmZJRdzIgpMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0183563-2608-4965-f613-c88b415178e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained decay rate of the first layer:\n",
            "Parameter containing:\n",
            "tensor(0.8183, requires_grad=True)\n",
            "Trained decay rates of the second layer: Parameter containing:\n",
            "tensor([0.8946, 0.8081, 0.8503], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Trained decay rate of the first layer:\")\n",
        "print(net.lif1.beta)\n",
        "\n",
        "print(f\"Trained decay rates of the second layer: {net.lif2.beta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXCggOzk2vYF"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec, _ = net(data)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ias_TerdCMoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984e96e2-e940-4093-bd32-73956cc3e252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 54.699%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test set accuracy: {test_accuracy(test_loader, net, num_steps)*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPvVZDZqVZ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3cPcNwflFqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A56nBQsMlGHH"
      },
      "source": [
        "# Preparing Data Loaders for CE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9966b2e9-ea3e-4ac4-e08d-2d04b36d2c77",
        "id": "NfTWCov_lGHL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ifvkV4mlGHN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf4e430-4f18-408f-e0d5-6fe85541d640",
        "id": "WmMAIshAlGHO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shared drives/Master Project\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18588, 30, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "%cd /content/drive/Shared drives/Master Project/\n",
        "\n",
        "with open('master_data_X.npy', 'rb') as f:\n",
        "  X = np.load(f)\n",
        "\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157c963b-536d-4177-ad9b-56a65926ccff",
        "id": "4T-Au1n-lGHP"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18588,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "with open('master_data_Y.npy', 'rb') as f:\n",
        "  y = np.load(f)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38d750a-db6a-44b5-89aa-f38a8fe5e13e",
        "id": "ccH7dAAZlGHQ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c481c4-a768-4e88-f56f-3042b52a3937",
        "id": "546vDgb3lGHR"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46623a96-5c87-4a5d-ccc7-7ad55e076214",
        "id": "gNDLh_AclGHS"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#X = np.int_(X)\n",
        "X = np.float32(X)\n",
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfd5847-8752-461f-ec5d-2a743b82ffb7",
        "id": "AI5_H201lGHT"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "y = np.float32(y)\n",
        "#y = np.int_(y)\n",
        "y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GDLlynNxlGHU"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JDCn1yHBlGHV"
      },
      "outputs": [],
      "source": [
        "x_set = torch.from_numpy(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d3b5e7-10ea-43bd-bb55-8d6204ce595a",
        "id": "XOHcOeI0lGHV"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18588, 30, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_set.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OFT13kSBmuRO",
        "outputId": "7bab27b9-8491-4b58-995a-26913a561da0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "L8ZYQrvqlGHW"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.type(torch.LongTensor)\n",
        "#print(type(x_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nZrodJATlGHX"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.unsqueeze(1)\n",
        "#x_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LFHHfDyglGHY"
      },
      "outputs": [],
      "source": [
        "#x_set = x_set.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e44e86-0566-4286-fcc7-fde4cf832869",
        "id": "atcdQoGrlGHZ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18588])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "y_set = torch.from_numpy(y)\n",
        "y_set.size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_set.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L1MVvkf1m6DA",
        "outputId": "fe23b637-53eb-4bbc-d274-a0b3f4be6f3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JaR9i5tPlGHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec1ea2e8-2072-4ce2-e37a-5f73806e1498"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.LongTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "y_set = y_set.type(torch.LongTensor)\n",
        "y_set.type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XpIRLCl-lGHd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sNZeox09lGHd"
      },
      "outputs": [],
      "source": [
        "data_set = TensorDataset(x_set, y_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GomSrsqelGHe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c46081-9435-4788-fbe6-1bd1eb5d3402",
        "id": "yUILAL1WlGHf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size:18588\n"
          ]
        }
      ],
      "source": [
        "test_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(x_set)\n",
        "print(\"Dataset size:\"+str(dataset_size))\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(test_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b63296a-defb-4b03-ee25-8cf2d72141b5",
        "id": "rDDqrSrJlGHh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13012\n"
          ]
        }
      ],
      "source": [
        "print(len(train_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334d52fc-5d11-4c47-9ebd-c295db72e4d6",
        "id": "qk0vkQJtlGHh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5576\n"
          ]
        }
      ],
      "source": [
        "print(len(test_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "otqskRR-lGHi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XtuhBR8blGHj"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "train_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
        "                                           sampler=train_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0dQWSwyDlGHk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNN model with CE"
      ],
      "metadata": {
        "id": "JgUnmSBblNNP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb04501-8fdc-4a58-e2a6-0f121525c7d1",
        "id": "9ODYh0fpm4Y9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/109.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "309tyoKjm4Y_"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "import torch.nn.functional as F\n",
        "import snntorch.functional as SF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ep3c3jz5m4ZA"
      },
      "outputs": [],
      "source": [
        "#batch_size = 128\n",
        "#data_path='/data/mnist'\n",
        "dtype=torch.long\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    beta = 0.9\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 8, 5, padding=\"same\")\n",
        "    self.lif1 = snn.Leaky(beta=beta)\n",
        "    self.mp1 = nn.MaxPool2d(2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(8, 1, 5, padding=\"same\")\n",
        "    self.lif2 = snn.Leaky(beta=beta)\n",
        "    self.mp2 = nn.MaxPool2d(2)\n",
        "\n",
        "    self.fc = nn.Linear(1750, 3)\n",
        "    self.lif3 = snn.Leaky(beta=beta)\n",
        "    #self.to(torch.long)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "    mem1 = self.lif1.init_leaky()\n",
        "    mem2 = self.lif2.init_leaky()\n",
        "    mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "    spk3_rec = []\n",
        "    mem3_rec = []\n",
        "\n",
        "        # time-loop\n",
        "    for step in range(num_steps):\n",
        "      cur1 = self.conv1(x)\n",
        "      spk1, mem1 = self.lif1(self.mp1(cur1), mem1)\n",
        "      cur2 = self.conv2(spk1)\n",
        "      spk2, mem2 = self.lif2(self.mp2(cur2), mem2)\n",
        "      cur3 = self.fc(spk2.flatten(1))\n",
        "      spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "      spk3_rec.append(spk3)\n",
        "      mem3_rec.append(mem3)\n",
        "    return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "convnet = ConvNet().to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4-7E0QsElPRK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = SF.ce_rate_loss()\n",
        "optimizer = torch.optim.Adam(convnet.parameters(), lr=1e-2, betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "m9vSSWl-mZaJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "num_steps = 10 #10\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data, targets in train_batch:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        convnet.train()\n",
        "        spk_rec, _ = convnet(data)\n",
        "        #print(spk_rec)\n",
        "        #print(counter)\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        #print(len(spk_rec.sum(0)))\n",
        "        #print(torch.sum(spk_rec, 1))\n",
        "        #print(targets.size())\n",
        "        #print(torch.sum(spk_rec.sum(0), (1)).size())\n",
        "        #print(spk_rec.sum(1))\n",
        "        #print(spk_rec.sum(2))\n",
        "\n",
        "\n",
        "        #loss_val = loss(spk_rec.sum(0), targets)\n",
        "        #loss_val = loss(torch.sum(spk_rec.sum(0), (1)), targets)\n",
        "        loss_val = loss(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "         # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "        acc = SF.accuracy_rate(spk_rec, targets)\n",
        "        acc_hist.append(acc)\n",
        "\n",
        "        # Print train/test loss/accuracy\n",
        "        if counter % 10 == 0:\n",
        "            print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()} \\t Accuracy: {acc * 100:.2f}\")\n",
        "        counter += 1\n",
        "\n",
        "        #if counter == 100:\n",
        "        #  break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mMdBLtumb8F",
        "outputId": "795a5447-3018-4a86-d8f6-444dd0dde18a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Train Loss: 1.0891789197921753 \t Accuracy: 100.00\n",
            "Iteration: 10 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 20 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 30 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 40 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 50 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 60 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 70 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 80 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 90 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 130 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 160 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 180 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 200 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 220 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 230 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 240 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 260 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 270 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 290 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 330 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 340 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 360 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 370 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 380 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 390 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 400 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 410 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 420 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 430 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 440 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 450 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 460 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 470 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 480 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 490 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 500 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 510 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 520 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 530 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 540 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 550 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 560 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 570 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 580 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 590 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 600 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 610 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 620 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 630 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 640 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 650 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 660 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 670 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 680 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 690 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 700 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 710 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 720 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 730 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 740 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 750 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 760 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 770 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 780 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 790 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 800 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 810 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 820 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 830 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 840 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 850 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 860 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 870 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 880 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 890 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 900 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 910 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 920 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 930 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 940 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 950 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 960 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 970 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 980 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 990 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1000 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1010 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1020 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1030 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1040 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1050 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1060 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1070 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1080 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1090 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1100 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1110 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1120 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1130 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1140 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1150 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1160 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1170 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1180 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1190 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1200 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1210 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1220 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1230 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1240 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1250 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1260 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1270 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1280 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1290 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1300 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1310 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1320 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1330 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1340 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1350 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1360 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1370 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1380 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1390 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1400 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1410 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1420 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1430 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1440 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1450 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1460 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1470 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1480 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1490 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1500 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1510 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1520 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1530 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1540 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1550 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1560 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1570 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1580 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1590 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1600 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1610 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1620 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1630 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1640 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1650 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1660 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1670 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1680 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1690 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1700 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1710 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1720 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1730 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1740 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1750 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1760 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1770 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1780 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1790 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1800 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1810 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1820 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1830 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1840 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1850 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1860 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1870 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1880 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1890 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1900 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1910 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1920 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1930 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 1940 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1950 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1960 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 1970 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1980 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 1990 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 2000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2010 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2020 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2040 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2060 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2070 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2080 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2090 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2130 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2160 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2180 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2190 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2200 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2220 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2230 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2240 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2250 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2260 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2270 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2290 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2320 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2360 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2370 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2380 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2390 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2400 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2410 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2430 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2440 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2450 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2460 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2470 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2500 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2520 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2530 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2550 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2560 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2570 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2590 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2600 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2620 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2630 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2640 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2650 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2660 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2670 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2700 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2710 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2720 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2730 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2740 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2750 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2760 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2770 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2780 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2790 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2850 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2860 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2870 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2890 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2910 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2920 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2940 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2950 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2970 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 2980 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 2990 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3010 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3020 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3030 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3040 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3060 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3070 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3080 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3090 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3130 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3160 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3180 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3190 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3210 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3220 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3230 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3240 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3260 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3270 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3290 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3300 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3340 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3360 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3370 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3380 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3390 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3400 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3410 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3430 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3440 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3460 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3470 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3480 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3500 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3520 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3530 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3540 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3560 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3570 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3600 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3620 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3660 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3670 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3680 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3700 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3710 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3720 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3730 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3740 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3750 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3760 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3770 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3780 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3790 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 3810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 3830 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3840 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3850 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3860 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3870 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 3880 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3890 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3900 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3910 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3920 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3930 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3940 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3950 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3960 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 3970 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3980 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 3990 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4000 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4010 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4020 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4030 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4040 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4050 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4060 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4070 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4080 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4090 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4100 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4110 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4120 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4130 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4140 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4150 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4160 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4170 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4180 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4190 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4200 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4210 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4220 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4230 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4240 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4250 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4260 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4270 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4280 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4290 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4300 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4310 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4320 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4330 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4340 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4350 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4360 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4370 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4380 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4390 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4400 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4410 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4420 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4430 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4440 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4450 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4460 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4470 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4480 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4490 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4500 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4510 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4520 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4530 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4540 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4550 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4560 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4570 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4580 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4590 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4600 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4610 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4620 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4630 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4640 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4650 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4660 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4670 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4680 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4690 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4700 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4710 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4720 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4730 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4740 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4750 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4760 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4770 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4780 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4790 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4800 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4810 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4820 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4830 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4840 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4850 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4860 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4870 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4880 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4890 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4900 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4910 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4920 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 4930 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4940 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 4950 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4960 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4970 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4980 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 4990 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5000 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5010 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5020 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5030 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5040 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5050 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5060 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5070 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5080 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5090 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5100 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 5110 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5120 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5130 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5140 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5150 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5160 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5170 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5180 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5190 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5200 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5210 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5220 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5230 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5240 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5250 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5260 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5270 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5280 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 5290 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5300 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5310 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5320 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 5330 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5340 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5350 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5360 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5370 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5380 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5390 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5400 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5410 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 5420 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5430 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5440 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5450 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5460 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5470 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5480 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5490 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5500 \t Train Loss: 0.8619948625564575 \t Accuracy: 0.00\n",
            "Iteration: 5510 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5520 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5530 \t Train Loss: 1.861994981765747 \t Accuracy: 0.00\n",
            "Iteration: 5540 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5550 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5560 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5570 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5580 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5590 \t Train Loss: 0.8619948625564575 \t Accuracy: 100.00\n",
            "Iteration: 5600 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5620 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5660 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5670 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5700 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5710 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5720 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5730 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5740 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5750 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5760 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5770 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5780 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5790 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5850 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5890 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5910 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5920 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5940 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5970 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 5980 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 5990 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6010 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6020 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6040 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6060 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6070 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6080 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6090 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6110 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6120 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6130 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6140 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6150 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6160 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6170 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6180 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6190 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6210 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6220 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6230 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6240 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6260 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6270 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6280 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6290 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6360 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6370 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6380 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6390 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6400 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6410 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6420 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6430 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6440 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6460 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6470 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6500 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6520 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6530 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6560 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6570 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6580 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6590 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6600 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6620 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6630 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6660 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6670 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6680 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6690 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6700 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6710 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6720 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6730 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6740 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6750 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6760 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6770 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6780 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6790 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6800 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6810 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6840 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6850 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6860 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6890 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6910 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6920 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6940 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6970 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 6980 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 6990 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7010 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7020 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7040 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7050 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7060 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7070 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7080 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7090 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7130 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7160 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7170 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7180 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7220 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7230 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7240 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7260 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7270 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7280 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7290 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7350 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7360 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7370 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7380 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7390 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7400 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7410 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7420 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7430 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7440 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7460 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7470 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7500 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7510 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7520 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7530 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7550 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7560 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7570 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7600 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7620 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7650 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7660 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7670 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7680 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7700 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7710 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7720 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7730 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7740 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7750 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7760 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7770 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7780 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7790 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7820 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7850 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7890 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7910 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7920 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7940 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7970 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 7980 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 7990 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8000 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8010 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8020 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8040 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8060 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8070 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8080 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8090 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8130 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8160 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8180 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8220 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8230 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8240 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8260 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8270 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8290 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8310 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8330 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8350 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8360 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8370 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8380 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8390 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8400 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8410 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8430 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8440 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8460 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8470 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8490 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8500 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8510 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8520 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8530 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8540 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8560 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8570 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8590 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8600 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8620 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8660 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8670 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8690 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8700 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8710 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8720 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8730 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8740 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8750 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8760 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8770 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8780 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8790 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8810 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8830 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8840 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8850 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8890 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8910 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8920 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8930 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8940 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8970 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 8980 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 8990 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9000 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9010 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9020 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9040 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9060 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9070 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9080 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9090 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9100 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9130 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9140 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9160 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9180 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9220 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9230 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9240 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9250 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9260 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9270 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9280 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9290 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9310 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9340 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9360 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9370 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9380 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9390 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9400 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9410 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9430 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9440 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9460 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9470 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9480 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9500 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9520 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9530 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9540 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9550 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9560 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9570 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9600 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 9610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9620 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 9630 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9640 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9650 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9660 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9670 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9680 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9690 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9700 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9710 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9720 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9730 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9740 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9750 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9760 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9770 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9780 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9790 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9800 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9810 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9820 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9830 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9840 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9850 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9860 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9870 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9880 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9890 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9900 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9910 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9920 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9930 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9940 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9950 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9960 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9970 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 9980 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 9990 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10000 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10010 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10020 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10030 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10040 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10050 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10060 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10070 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10080 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10090 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10100 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10110 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10120 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10130 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10140 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10150 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10160 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10170 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10180 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10190 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10200 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10210 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10220 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10230 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10240 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10250 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10260 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10270 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10280 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10290 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10300 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10310 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10320 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10330 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10340 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10350 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10360 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10370 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10380 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10390 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10400 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10410 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10420 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10430 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10440 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10450 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10460 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10470 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10480 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10490 \t Train Loss: 1.0986121892929077 \t Accuracy: 0.00\n",
            "Iteration: 10500 \t Train Loss: 1.0986121892929077 \t Accuracy: 100.00\n",
            "Iteration: 10510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10520 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10530 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10560 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10570 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10600 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10610 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10620 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10650 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10660 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10670 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10700 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10710 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10720 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10730 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10740 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10750 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10760 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10770 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10780 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10790 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10800 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10850 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10880 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10890 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10910 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10920 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10940 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 10970 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10980 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 10990 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11010 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11020 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11030 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11040 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11050 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11060 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11070 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11080 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11090 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11100 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11110 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11130 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11140 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11160 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11170 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11180 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11200 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11210 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11220 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11230 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11240 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11260 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11270 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11290 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11310 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11320 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11330 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11350 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11360 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11370 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11380 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11390 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11400 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11410 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11430 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11440 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11450 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11460 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11470 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11500 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11510 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11520 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11530 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11560 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11570 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11580 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11600 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11620 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11640 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11660 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11670 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11690 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11700 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11710 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11720 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11730 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11740 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11750 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11760 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11770 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11780 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11790 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11800 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11820 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11830 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11850 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11870 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11880 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11890 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11900 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11910 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11920 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11930 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 11940 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11970 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11980 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 11990 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12000 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12010 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12020 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12030 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12040 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12050 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12060 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12070 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12080 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12090 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12100 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12110 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12120 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12130 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12140 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12150 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12160 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12170 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12180 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12190 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12200 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12210 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12220 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12230 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12240 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12250 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12260 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12270 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12280 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12290 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12300 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12310 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12320 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12330 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12340 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12350 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12360 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12370 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12380 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12390 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12400 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12410 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12420 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12430 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12440 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12450 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12460 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12470 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12480 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12490 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12500 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12510 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12520 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12530 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12540 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12550 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12560 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12570 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12580 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12590 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12600 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12610 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12620 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12630 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12640 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12650 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12660 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12670 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12680 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12690 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12700 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12710 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12720 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12730 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12740 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12750 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12760 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12770 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12780 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12790 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12800 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12810 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12820 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12830 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12840 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12850 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12860 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12870 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12880 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12890 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12900 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12910 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 12920 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12930 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12940 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12950 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12960 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12970 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12980 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 12990 \t Train Loss: 1.5514448881149292 \t Accuracy: 0.00\n",
            "Iteration: 13000 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n",
            "Iteration: 13010 \t Train Loss: 0.5514448285102844 \t Accuracy: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(acc_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adwU-fAg7uBH",
        "outputId": "58b3c6b8-9445-480d-f1a1-174ebab278ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training Accuracy\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(acc_hist)\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Timesteps, Epoch = 1\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "-b9HDbGv1lq5",
        "outputId": "0a79ddc0-a18c-48bc-babc-7a79344a56ea"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Training Loss\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(loss_hist)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Timesteps for Epoch = 1\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0OU2R-nI1ru-",
        "outputId": "3e6a1e72-c62e-4d4e-a16b-c5a460c040e9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_accuracy(model, dataloader):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    running_length = 0\n",
        "    running_accuracy = 0\n",
        "\n",
        "    for data, targets in iter(dataloader):\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward-pass\n",
        "      spk_rec, _ = model(data)\n",
        "      spike_count = spk_rec.sum(0)\n",
        "      _, max_spike = spike_count.max(1)\n",
        "\n",
        "      # correct classes for one batch\n",
        "      num_correct = (max_spike == targets).sum()\n",
        "\n",
        "      # total accuracy\n",
        "      running_length += len(targets)\n",
        "      running_accuracy += num_correct\n",
        "\n",
        "    accuracy = (running_accuracy / running_length)\n",
        "\n",
        "    return accuracy.item()\n"
      ],
      "metadata": {
        "id": "_8bfmpz9mmUQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ConvNet Accuracy: {measure_accuracy(convnet, test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmWnUnpT6KAJ",
        "outputId": "1e3b0b37-6468-4fbc-fac2-9195ddd62563"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet Accuracy: 0.547345757484436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjQOej8z1aLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "HNxWksq0Bi13"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}